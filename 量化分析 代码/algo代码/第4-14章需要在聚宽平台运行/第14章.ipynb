{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#首先导入必要的库\n",
    "#有些库读者朋友可能不知道是做什么的\n",
    "#没有关系，后面我们在用到的时候，会进行讲解\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Activation, Input\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9214 entries, 0 to 9213\n",
      "Data columns (total 2 columns):\n",
      "text        9214 non-null object\n",
      "polarity    9214 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 144.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#这个单元格中的内容就是在第12、13章中用过的\n",
    "#载入数据并添加极性标签\n",
    "#并合成一个DataFrame的代码\n",
    "#本章中就不逐行注释了\n",
    "pos_corpus = []\n",
    "with open('positive.txt','r') as f:\n",
    "    for sent in f:\n",
    "        pos_corpus.append(sent.replace('\\n', ''))\n",
    "neg_corpus = []\n",
    "with open('negtive.txt', 'r') as f:\n",
    "    for sent in f:\n",
    "        neg_corpus.append(sent.replace('\\n', ''))\n",
    "pos_df = pd.DataFrame(pos_corpus, columns=['text'])\n",
    "pos_df['polarity'] = 1\n",
    "neg_df = pd.DataFrame(neg_corpus, columns=['text'])\n",
    "neg_df['polarity'] = 0\n",
    "df = pd.concat([pos_df, neg_df]).reset_index(drop = True)\n",
    "#检查一下DataFrame的信息\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分配好数据集的特征和目标\n",
    "X = df['text']\n",
    "y = df['polarity'].astype('int')\n",
    "#使用tokenizer对数据进行处理\n",
    "#这个在第13章中，也是使用过的了\n",
    "tokenizer = Tokenizer(filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                     lower = True, split=\" \")\n",
    "#用tokenizer拟合文本数据\n",
    "tokenizer.fit_on_texts(X)\n",
    "#文本特征存储在word_index中\n",
    "vocab = tokenizer.word_index\n",
    "#拆分数据\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(X, y, random_state = 30)\n",
    "#这次我们使用填充序列来训练模型\n",
    "#也就是用pad_sequences来进行处理\n",
    "X_train_word_ids = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将训练集和验证集都转化为填充序列\n",
    "#为了节省时间，我们设置序列的最大长度为16\n",
    "X_train_padded_seqs = pad_sequences(X_train_word_ids, maxlen=16)\n",
    "X_test_padded_seqs = pad_sequences(X_test_word_ids, maxlen=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 16, 8)        125160      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 16, 16)       400         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 16, 16)       528         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 16, 16)       656         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2, 16)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2, 16)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 2, 16)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 48)        0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 96)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 96)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            97          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 126,841\n",
      "Trainable params: 126,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#下面我们就开始搭建卷积神经网络\n",
    "#首先是建立一个输入，因为填充序列的长度是16\n",
    "#所以Input的形态也要指定为16，数据类型为64位浮点数\n",
    "main_input = Input(shape = (16,),dtype = 'float64')\n",
    "#这里我们引入一个嵌入层，对输入的序列进行处理\n",
    "embedder = Embedding(len(vocab)+1, 8, input_length = 16)\n",
    "embed = embedder(main_input)\n",
    "#先创建一个1维卷积神经层\n",
    "cnn1 = Convolution1D(16, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "#用一个池化层与cnn1堆叠\n",
    "cnn1 = MaxPool1D(pool_size=8)(cnn1)\n",
    "#创建第二个1维卷积层\n",
    "cnn2 = Convolution1D(16, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "#同样与池化层堆叠\n",
    "cnn2 = MaxPool1D(pool_size=8)(cnn2)\n",
    "#第3个1维卷积层\n",
    "cnn3 = Convolution1D(16, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "#与池化层堆叠\n",
    "cnn3 = MaxPool1D(pool_size=8)(cnn3)\n",
    "#将3个卷积层进行连接\n",
    "cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "#使用一个Flatten层，把输入从高维压缩到1维\n",
    "flat = Flatten()(cnn)\n",
    "#添加一个dropout层来进行正则化\n",
    "drop = Dropout(0.2)(flat)\n",
    "#最后是一个全连接层，用来输出模型结果\n",
    "main_output = Dense(1, activation='sigmoid')(drop)\n",
    "#这次使用Model来搭建模型，输入和输出分别是最初的输入和全连接层给出的输出\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "#最后对模型进行编译\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#查看模型的概述\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6910 samples, validate on 2304 samples\n",
      "Epoch 1/10\n",
      "6910/6910 [==============================] - 1s 179us/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.4316 - val_acc: 0.8659\n",
      "Epoch 2/10\n",
      "6910/6910 [==============================] - 1s 149us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.4402 - val_acc: 0.8628\n",
      "Epoch 3/10\n",
      "6910/6910 [==============================] - 1s 133us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.4474 - val_acc: 0.8628\n",
      "Epoch 4/10\n",
      "6910/6910 [==============================] - 1s 141us/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.4607 - val_acc: 0.8585\n",
      "Epoch 5/10\n",
      "6910/6910 [==============================] - 1s 133us/step - loss: 0.0046 - acc: 0.9993 - val_loss: 0.4676 - val_acc: 0.8585\n",
      "Epoch 6/10\n",
      "6910/6910 [==============================] - 1s 143us/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.4742 - val_acc: 0.8568\n"
     ]
    }
   ],
   "source": [
    "#首先设置early_stopping，\n",
    "#这次选择监控的指标是验证集的准确率，\n",
    "#在准确率连续下降5次后停止训练\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "#设置模型的检查点，用来保存最佳的模型参数\n",
    "model_checkpoint = ModelCheckpoint('model-TextCNN.h5', save_best_only=True)\n",
    "#下面就开模型的训练\n",
    "#为了节约时间，还是将轮次设定为10\n",
    "hist = model.fit(X_train_padded_seqs, y_train, batch_size=128, epochs=10,\n",
    "                 validation_data=(X_test_padded_seqs, y_test),\n",
    "                 callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 0s 53us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4742104259009163, 0.8567708333333334]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_padded_seqs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9986297]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用卷积神经网络模型对样本作出预测\n",
    "model.predict(X_test_padded_seqs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3596    上证指数 创业板 指 任性 机会 不 好好 把握\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#清除一下垃圾\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 16, 8)             125160    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 125,713\n",
      "Trainable params: 125,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#下面来搭建长短期记忆网络\n",
    "lstm = Sequential()\n",
    "#在网络中先添加一个Embedding层\n",
    "lstm.add(Embedding(len(vocab)+1, 8, weights=[np.zeros((len(vocab) + 1, 8))], \n",
    "                   input_length=16, trainable=True))\n",
    "#添加长短期记忆网络\n",
    "lstm.add(LSTM(8, dropout=0.5, recurrent_dropout=0.2))\n",
    "#添加全连接层\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "#编译模型\n",
    "lstm.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "#看产模型的概况\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6910 samples, validate on 2304 samples\n",
      "Epoch 1/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.2921 - acc: 0.9195 - val_loss: 0.3520 - val_acc: 0.8655\n",
      "Epoch 2/10\n",
      "6910/6910 [==============================] - 9s 1ms/step - loss: 0.2304 - acc: 0.9368 - val_loss: 0.3284 - val_acc: 0.8724\n",
      "Epoch 3/10\n",
      "6910/6910 [==============================] - 9s 1ms/step - loss: 0.1878 - acc: 0.9479 - val_loss: 0.3183 - val_acc: 0.8715\n",
      "Epoch 4/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.1539 - acc: 0.9608 - val_loss: 0.3088 - val_acc: 0.8785\n",
      "Epoch 5/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.1256 - acc: 0.9671 - val_loss: 0.3061 - val_acc: 0.8841\n",
      "Epoch 6/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.1084 - acc: 0.9750 - val_loss: 0.3131 - val_acc: 0.8841\n",
      "Epoch 7/10\n",
      "6910/6910 [==============================] - 9s 1ms/step - loss: 0.0931 - acc: 0.9776 - val_loss: 0.3175 - val_acc: 0.8845\n",
      "Epoch 8/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.0789 - acc: 0.9819 - val_loss: 0.3270 - val_acc: 0.8832\n",
      "Epoch 9/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.0719 - acc: 0.9836 - val_loss: 0.3300 - val_acc: 0.8824\n",
      "Epoch 10/10\n",
      "6910/6910 [==============================] - 10s 1ms/step - loss: 0.0656 - acc: 0.9870 - val_loss: 0.3380 - val_acc: 0.8811\n"
     ]
    }
   ],
   "source": [
    "#这里是设置模型停止和保存检查点的代码\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('model-LSTM.h5', save_best_only=True)\n",
    "#开始训练LSTM网络\n",
    "hist = lstm.fit(X_train_padded_seqs, y_train,\n",
    "                batch_size=128,\n",
    "                epochs=10,\n",
    "                validation_data=(X_test_padded_seqs, y_test),\n",
    "                callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304/2304 [==============================] - 1s 624us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33796224743127823, 0.8810763888888888]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.evaluate(X_test_padded_seqs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9755633]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.predict(X_test_padded_seqs[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 276, 60, 229, 2951, 38, 1, 786, 478]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded_seqs[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 276, 60, 229, 2951, 38, 1, 786, 478]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_padded_seqs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras的模型保存是比较简单的\n",
    "#使用save方法就可以了\n",
    "lstm.save('lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "MarkDown菜单",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
